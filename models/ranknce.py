from packaging import version
import torch
from torch import nn
import torch.nn.functional as F


class RankNCELoss(nn.Module):
    """
    RankNCE: Exploring Negatives in Contrastive Learning for Unpaired Image-to-Image Translation
    æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡äº’ä¿¡æ¯è´¡çŒ®æ’åºï¼Œé€‰æ‹©é«˜è´¨é‡è´Ÿæ ·æœ¬ï¼Œæ’é™¤å‡é˜´æ€§ï¼ˆFalse Negativesï¼‰
    
    ç­–ç•¥ï¼š
    1. æ’é™¤ç›¸ä¼¼åº¦æœ€é«˜çš„è´Ÿæ ·æœ¬ï¼ˆå¯èƒ½æ˜¯å‡é˜´æ€§ï¼Œå¦‚åŒä¸€ç‰©ä½“çš„ä¸åŒéƒ¨åˆ†ï¼‰
    2. æ’é™¤ç›¸ä¼¼åº¦æœ€ä½çš„è´Ÿæ ·æœ¬ï¼ˆæ— ä¿¡æ¯ï¼Œè¿‡äºç®€å•ï¼‰
    3. ä¿ç•™ä¸­ç­‰éš¾åº¦çš„è´Ÿæ ·æœ¬ï¼ˆæœ€å…·åŒºåˆ†æ€§ï¼‰
    """
    def __init__(self, opt, top_k_ratio=0.5, bottom_k_ratio=0.1):
        super().__init__()
        self.opt = opt
        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='none')
        self.mask_dtype = torch.uint8 if version.parse(torch.__version__) < version.parse('1.2.0') else torch.bool
        
        # RankNCE ç‰¹å®šå‚æ•°
        self.top_k_ratio = getattr(opt, 'ranknce_top_k', top_k_ratio)
        self.bottom_k_ratio = getattr(opt, 'ranknce_bottom_k', bottom_k_ratio)
        
        # ç¡®ä¿æ¯”ä¾‹åˆç†
        assert 0 <= self.bottom_k_ratio < self.top_k_ratio <= 1.0, \
            f"Invalid ratios: bottom_k={self.bottom_k_ratio}, top_k={self.top_k_ratio}"
        
        print(f"[RankNCE] top_k_ratio={self.top_k_ratio:.2f}, "
              f"bottom_k_ratio={self.bottom_k_ratio:.2f}, "
              f"effective_ratio={self.top_k_ratio - self.bottom_k_ratio:.2f}")
        
    def forward(self, feat_q, feat_k):
        """
        Args:
            feat_q: query features [num_patches, dim] æˆ– [batch, num_patches, dim]
            feat_k: key features [num_patches, dim] æˆ– [batch, num_patches, dim]
        """
        batch_dim = len(feat_q.shape)
        temperature = getattr(self.opt, 'nce_T', 0.07)
        
        if batch_dim == 2:
            # å•å¼ å›¾ç‰‡æƒ…å†µï¼š[num_patches, dim]
            feat_k = feat_k.detach()
            num_patches = feat_q.shape[0]
            
            # è®¡ç®—æ­£æ ·æœ¬ logit: [num_patches, 1]
            l_pos = torch.sum(feat_q * feat_k, dim=1, keepdim=True)
            
            # è®¡ç®—æ‰€æœ‰è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦: [num_patches, num_patches]
            sim_matrix = torch.mm(feat_q, feat_k.t())
            
            # æ’é™¤å¯¹è§’çº¿ï¼ˆæ­£æ ·æœ¬ï¼‰
            mask = torch.eye(num_patches, device=sim_matrix.device, dtype=torch.bool)
            sim_matrix = sim_matrix.masked_fill(mask, float('-inf'))
            
            # é€‰æ‹©é«˜è´¨é‡è´Ÿæ ·æœ¬
            out = self._select_ranked_negatives(sim_matrix, l_pos)
            
        else:
            # batch æƒ…å†µï¼š[batch, num_patches, dim]
            batch_size, npatches, dim = feat_q.shape
            
            if self.opt.nce_includes_all_negatives_from_minibatch:
                # æ‰€æœ‰ batch å†…æ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬
                # feat_q/feats_k: [batch, npatches, dim] -> [1, batch*npatches, dim]
                feat_q = feat_q.view(1, -1, dim)
                feat_k = feat_k.view(1, -1, dim).detach()
                total_patches = batch_size * npatches
                
                # æ­£æ ·æœ¬ logit: å¯¹åº”ä½ç½®
                l_pos = torch.sum(feat_q.view(total_patches, dim) * feat_k.view(total_patches, dim), 
                                dim=1, keepdim=True)  # [total_patches, 1]
                
                # æ‰€æœ‰ç›¸ä¼¼åº¦: [total_patches, total_patches]
                sim_matrix = torch.mm(feat_q.view(total_patches, dim), 
                                    feat_k.view(total_patches, dim).t())
                
                # æ’é™¤æ­£æ ·æœ¬ï¼šå¯¹è§’çº¿
                mask = torch.eye(total_patches, device=sim_matrix.device, dtype=torch.bool)
                sim_matrix = sim_matrix.masked_fill(mask, float('-inf'))
                
                out = self._select_ranked_negatives(sim_matrix, l_pos)
                
            else:
                # æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å¤„ç†
                feat_k = feat_k.detach()
                
                # æ­£æ ·æœ¬ logit: [batch, npatches, 1]
                l_pos = torch.sum(feat_q * feat_k, dim=2, keepdim=True)
                
                # æ‰€æœ‰ç›¸ä¼¼åº¦: [batch, npatches, npatches]
                sim_matrix = torch.bmm(feat_q, feat_k.transpose(2, 1))
                
                # æ’é™¤å¯¹è§’çº¿ï¼ˆæ¯ä¸ªæ ·æœ¬å†…éƒ¨ï¼‰
                mask = torch.eye(npatches, device=sim_matrix.device, dtype=torch.bool).unsqueeze(0)
                sim_matrix = sim_matrix.masked_fill(mask, float('-inf'))
                
                # reshape ä¸º [batch*npatches, npatches] ç»Ÿä¸€å¤„ç†
                sim_matrix = sim_matrix.view(-1, npatches)
                l_pos = l_pos.view(-1, 1)
                
                out = self._select_ranked_negatives(sim_matrix, l_pos)
        
        # æ¸©åº¦ç¼©æ”¾
        out = out / temperature
        
        # è®¡ç®—æŸå¤±ï¼ˆæ ‡ç­¾æ˜¯ 0ï¼Œå³ç¬¬ä¸€åˆ—æ˜¯æ­£æ ·æœ¬ï¼‰
        loss = self.cross_entropy_loss(
            out, 
            torch.zeros(out.size(0), dtype=torch.long, device=feat_q.device)
        )
        
        return loss
    
    def _select_ranked_negatives(self, sim_matrix, l_pos):
        """
        å‘é‡åŒ–å®ç°ï¼šåŸºäºæ’åºé€‰æ‹©é«˜è´¨é‡è´Ÿæ ·æœ¬
        ä½¿ç”¨ topk é¿å… -inf é—®é¢˜
        
        Args:
            sim_matrix: [N, M] è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆå¯¹è§’çº¿å·² mask ä¸º -infï¼‰
            l_pos: [N, 1] æ­£æ ·æœ¬ logit
        Returns:
            out: [N, 1+k_select]
        """
        # 1. è·å–æœ‰æ•ˆè´Ÿæ ·æœ¬æ•°é‡ï¼ˆæ’é™¤ -infï¼‰
        valid_mask = sim_matrix != float('-inf')
        num_valid_per_query = valid_mask.sum(dim=1)  # [N]
        min_valid = num_valid_per_query.min().item()
        
        if min_valid == 0:
            # æç«¯æƒ…å†µï¼šæ²¡æœ‰è´Ÿæ ·æœ¬
            return torch.cat([l_pos, torch.zeros_like(l_pos)], dim=1)
        
        # 2. è®¡ç®— k å€¼ï¼ˆåŸºäºæœ€å°æœ‰æ•ˆè´Ÿæ ·æœ¬æ•°ï¼‰
        k_top = max(1, min(int(min_valid * self.top_k_ratio), min_valid))
        k_bottom = min(int(min_valid * self.bottom_k_ratio), k_top - 1)
        k_select = k_top - k_bottom
        
        # ç¡®ä¿è‡³å°‘é€‰æ‹© 1 ä¸ªè´Ÿæ ·æœ¬
        if k_select < 1:
            k_select = 1
            k_bottom = k_top - 1
        
        # 3. ä½¿ç”¨ topk è·å–å‰ k_top ä¸ªæœ€ç›¸ä¼¼çš„è´Ÿæ ·æœ¬
        # topk ä¼šè‡ªåŠ¨è·³è¿‡ -infï¼ˆé™¤éæ‰€æœ‰å€¼éƒ½æ˜¯ -infï¼‰
        top_values, _ = torch.topk(sim_matrix, k=k_top, dim=1, largest=True, sorted=True)
        
        # 4. é€‰æ‹©æ’ååœ¨ [k_bottom, k_top) çš„è´Ÿæ ·æœ¬
        # æ’é™¤æœ€ç›¸ä¼¼çš„ k_bottom ä¸ªï¼ˆå¯èƒ½çš„å‡é˜´æ€§ï¼‰
        selected_neg_sim = top_values[:, k_bottom:]
        
        # 5. ç»„åˆæ­£è´Ÿæ ·æœ¬
        out = torch.cat([l_pos, selected_neg_sim], dim=1)
        
        return out


# ============= æµ‹è¯•ä»£ç  =============
if __name__ == "__main__":
    print("=" * 70)
    print("RankNCE Loss å®Œæ•´æµ‹è¯•")
    print("=" * 70)
    
    # æ¨¡æ‹Ÿé…ç½®
    class DummyOpt:
        nce_T = 0.07
        batch_size = 4
        nce_includes_all_negatives_from_minibatch = False
        ranknce_top_k = 0.5
        ranknce_bottom_k = 0.1
    
    # ========== æµ‹è¯• 1: å•å¼ å›¾ç‰‡ ==========
    print("\næµ‹è¯• 1: å•å¼ å›¾ç‰‡ [num_patches, dim]")
    opt = DummyOpt()
    criterion = RankNCELoss(opt)
    
    num_patches = 256
    dim = 256
    feat_q = F.normalize(torch.randn(num_patches, dim), dim=1)
    feat_k = F.normalize(torch.randn(num_patches, dim), dim=1)
    
    loss = criterion(feat_q, feat_k)
    print(f"  è¾“å…¥: feat_q={feat_q.shape}, feat_k={feat_k.shape}")
    print(f"  æŸå¤±: mean={loss.mean().item():.4f}, std={loss.std().item():.4f}")
    assert loss.shape == (num_patches,)
    print("  âœ… é€šè¿‡")
    
    # ========== æµ‹è¯• 2: Batch (ç‹¬ç«‹æ¨¡å¼) ==========
    print("\næµ‹è¯• 2: Batch [batch, num_patches, dim] - ç‹¬ç«‹æ¨¡å¼")
    opt.nce_includes_all_negatives_from_minibatch = False
    criterion = RankNCELoss(opt)
    
    batch = 4
    num_patches = 128
    feat_q = F.normalize(torch.randn(batch, num_patches, dim), dim=2)
    feat_k = F.normalize(torch.randn(batch, num_patches, dim), dim=2)
    
    loss = criterion(feat_q, feat_k)
    print(f"  è¾“å…¥: feat_q={feat_q.shape}, feat_k={feat_k.shape}")
    print(f"  æŸå¤±: mean={loss.mean().item():.4f}")
    assert loss.shape == (batch * num_patches,)
    print("  âœ… é€šè¿‡")
    
    # ========== æµ‹è¯• 3: Batch (å…±äº«æ¨¡å¼) ==========
    print("\næµ‹è¯• 3: Batch [batch, num_patches, dim] - å…±äº«è´Ÿæ ·æœ¬æ¨¡å¼")
    opt.nce_includes_all_negatives_from_minibatch = True
    criterion = RankNCELoss(opt)
    
    loss = criterion(feat_q, feat_k)
    print(f"  è¾“å…¥: feat_q={feat_q.shape}")
    print(f"  æŸå¤±: mean={loss.mean().item():.4f}")
    assert loss.shape == (batch * num_patches,)
    print("  âœ… é€šè¿‡")
    
    # ========== æµ‹è¯• 4: ä¸åŒå‚æ•° ==========
    print("\næµ‹è¯• 4: ä¸åŒ top_k/bottom_k é…ç½®")
    configs = [
        (1.0, 0.0, "ä½¿ç”¨æ‰€æœ‰è´Ÿæ ·æœ¬"),
        (0.5, 0.1, "æ ‡å‡†RankNCE"),
        (0.3, 0.0, "åªç”¨æœ€éš¾30%"),
    ]
    
    feat_q = F.normalize(torch.randn(128, 128), dim=1)
    feat_k = F.normalize(torch.randn(128, 128), dim=1)
    
    for top_k, bottom_k, desc in configs:
        opt_test = DummyOpt()
        opt_test.ranknce_top_k = top_k
        opt_test.ranknce_bottom_k = bottom_k
        opt_test.nce_includes_all_negatives_from_minibatch = False
        
        criterion_test = RankNCELoss(opt_test)
        loss = criterion_test(feat_q, feat_k)
        print(f"  {desc}: loss={loss.mean().item():.4f}")
    
    print("  âœ… é€šè¿‡")
    
    # ========== æµ‹è¯• 5: æ¢¯åº¦ ==========
    print("\næµ‹è¯• 5: æ¢¯åº¦æµ‹è¯•")
    opt = DummyOpt()
    opt.nce_includes_all_negatives_from_minibatch = False
    criterion = RankNCELoss(opt)
    
    # ğŸ”§ ä¿®å¤ï¼šå…ˆåˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡ï¼Œå†å½’ä¸€åŒ–
    feat_q_raw = torch.randn(128, 128, requires_grad=True)
    
    # å½’ä¸€åŒ–ï¼ˆè¿™ä¼šåˆ›å»ºæ–°å¼ é‡ï¼Œä½†æ¢¯åº¦ä¼šä¼ å› feat_q_rawï¼‰
    feat_q = F.normalize(feat_q_raw, dim=1)
    
    loss = criterion(feat_q, feat_k).mean()
    loss.backward()
    
    print(f"  æŸå¤±: {loss.item():.4f}")
    print(f"  feat_q_raw æ¢¯åº¦èŒƒæ•°: {feat_q_raw.grad.norm().item():.4f}")
    assert not torch.isnan(feat_q_raw.grad).any()
    assert feat_q_raw.grad.abs().sum() > 0, "æ¢¯åº¦åº”è¯¥éé›¶"
    print("  âœ… æ¢¯åº¦æ­£å¸¸ä¼ æ’­")
    
    # ========== æµ‹è¯• 6: è¾¹ç•Œæƒ…å†µ ==========
    print("\næµ‹è¯• 6: æç«¯å‚æ•° (top_k=0.95, bottom_k=0.9)")
    opt = DummyOpt()
    opt.ranknce_top_k = 0.95
    opt.ranknce_bottom_k = 0.9
    opt.nce_includes_all_negatives_from_minibatch = False
    criterion = RankNCELoss(opt)
    
    feat_q = F.normalize(torch.randn(64, 64), dim=1)
    feat_k = F.normalize(torch.randn(64, 64), dim=1)
    loss = criterion(feat_q, feat_k)
    print(f"  æŸå¤±: {loss.mean().item():.4f}")
    assert not torch.isnan(loss).any()
    assert not torch.isinf(loss).any()
    print("  âœ… é€šè¿‡")
    
    # ========== æµ‹è¯• 7: å°‘é‡ patches è¾¹ç•Œæƒ…å†µ ==========
    print("\næµ‹è¯• 7: å°‘é‡ patches (num_patches=8)")
    opt = DummyOpt()
    opt.nce_includes_all_negatives_from_minibatch = False
    criterion = RankNCELoss(opt)
    
    feat_q = F.normalize(torch.randn(8, 64), dim=1)
    feat_k = F.normalize(torch.randn(8, 64), dim=1)
    loss = criterion(feat_q, feat_k)
    print(f"  æŸå¤±: {loss.mean().item():.4f}")
    assert loss.shape == (8,)
    print("  âœ… é€šè¿‡")
    
    # ========== æµ‹è¯• 8: éªŒè¯é€‰æ‹©çš„è´Ÿæ ·æœ¬æ•°é‡ ==========
    print("\næµ‹è¯• 8: éªŒè¯é€‰æ‹©çš„è´Ÿæ ·æœ¬æ•°é‡")
    
    class InspectOpt:
        nce_T = 0.07
        batch_size = 1
        nce_includes_all_negatives_from_minibatch = False
        ranknce_top_k = 0.5
        ranknce_bottom_k = 0.1
    
    opt = InspectOpt()
    criterion = RankNCELoss(opt)
    
    num_patches = 100
    feat_q = F.normalize(torch.randn(num_patches, 64), dim=1)
    feat_k = F.normalize(torch.randn(num_patches, 64), dim=1)
    
    # æ‰‹åŠ¨è®¡ç®—æœŸæœ›çš„è´Ÿæ ·æœ¬æ•°
    num_negatives = num_patches - 1  # 99
    k_top = int(num_negatives * 0.5)  # 49
    k_bottom = int(num_negatives * 0.1)  # 9
    expected_neg_count = k_top - k_bottom  # 40
    
    # é€šè¿‡é’©å­æ£€æŸ¥è¾“å‡ºç»´åº¦
    loss = criterion(feat_q, feat_k)
    
    # å®é™…ä¸Šæˆ‘ä»¬å¯ä»¥é€šè¿‡ _select_ranked_negatives éªŒè¯
    l_pos = torch.sum(feat_q * feat_k, dim=1, keepdim=True)
    sim_matrix = torch.mm(feat_q, feat_k.t())
    mask = torch.eye(num_patches, device=sim_matrix.device, dtype=torch.bool)
    sim_matrix = sim_matrix.masked_fill(mask, float('-inf'))
    
    out = criterion._select_ranked_negatives(sim_matrix, l_pos)
    actual_neg_count = out.shape[1] - 1  # å‡å»æ­£æ ·æœ¬é‚£ä¸€åˆ—
    
    print(f"  æ€» patches: {num_patches}")
    print(f"  å¯ç”¨è´Ÿæ ·æœ¬: {num_negatives}")
    print(f"  æœŸæœ›é€‰æ‹©: {expected_neg_count}")
    print(f"  å®é™…é€‰æ‹©: {actual_neg_count}")
    assert actual_neg_count == expected_neg_count, \
        f"è´Ÿæ ·æœ¬æ•°é‡ä¸åŒ¹é…ï¼šæœŸæœ› {expected_neg_count}ï¼Œå®é™… {actual_neg_count}"
    print("  âœ… è´Ÿæ ·æœ¬æ•°é‡æ­£ç¡®")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼RankNCE å®ç°å®Œå…¨æ­£ç¡®")
    print("=" * 70)