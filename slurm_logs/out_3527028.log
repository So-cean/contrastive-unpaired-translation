----------------- Options ---------------
                 CUT_mode: CUT                           
               batch_size: 16                            	[default: 1]
                    beta1: 0.5                           
                    beta2: 0.999                         
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 256                           
                 csv_path: /public_bme2/bme-wangqian2/songhy2024/harmonization_mri/dataset/PVWMI_data.csv	[default: data.csv]
                 dataroot: /public_bme2/bme-wangqian2/songhy2024/data/PVWMI/T1w/k2I-SIEMENS-SKYRA-3.0T/	[default: placeholder]
             dataset_mode: monai                         	[default: unaligned]
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: 0                             	[default: None]
            display_ncols: 4                             
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
               easy_label: experiment_name               
                    epoch: latest                        
              epoch_count: 1                             
          evaluation_freq: 5000                          
        flip_equivariance: False                         
                 gan_mode: lsgan                         
                  gpu_ids: 0                             
                init_gain: 0.02                          
                init_type: xavier                        
                 input_nc: 1                             	[default: 3]
                  isTrain: True                          	[default: None]
               lambda_GAN: 1.0                           
               lambda_NCE: 1.0                           
              lambda_SSIM: 1.0                           
              lambda_grad: 0.5                           
                load_size: 256                           	[default: 286]
                       lr: 0.0001                        	[default: 0.0002]
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: cut                           
                 n_epochs: 50                            	[default: 200]
           n_epochs_decay: 50                            	[default: 200]
               n_layers_D: 3                             
                     name: CUT_monai_K2E2                	[default: experiment_name]
                    nce_T: 0.07                          
                  nce_idt: True                          
nce_includes_all_negatives_from_minibatch: False                         
               nce_layers: 0,4,8,12,16                   
                      ndf: 64                            
                     netD: basic                         
                     netF: mlp_sample                    
                  netF_nc: 256                           
                     netG: resnet_9blocks                
                      ngf: 96                            	[default: 64]
             no_antialias: False                         
          no_antialias_up: False                         
               no_dropout: True                          
                  no_flip: True                          	[default: False]
                  no_html: False                         
                    normD: instance                      
                    normG: instance                      
              num_patches: 256                           
              num_threads: 8                             	[default: 4]
                output_nc: 1                             	[default: 3]
                    phase: train                         
                pool_size: 0                             
               preprocess: scale_width_and_crop          	[default: resize_and_crop]
          pretrained_name: None                          
               print_freq: 100                           
         random_scale_max: 3.0                           
             save_by_iter: False                         
          save_epoch_freq: 20                            	[default: 5]
         save_latest_freq: 5000                          
           serial_batches: False                         
stylegan2_G_num_downsampling: 1                             
                   suffix:                               
         update_html_freq: 1000                          
                  verbose: False                         
----------------- End -------------------
Found 61 files in domain A (thick slices) and 51 files in domain B (thin slices)
Volume cache size: 100
Dataset will output 1 channel(s) per slice
Domain A: 4 CP=1, 57 CP=2
Domain B: 43 CP=1, 8 CP=2
Calculating valid slices for each domain...
Domain A (thick) total valid slices: 6123
Domain B (thin) total valid slices: 678
dataset [MonaiDataset] was created
model [CUTModel] was created
The number of training images = 6123
create web directory ./checkpoints/CUT_monai_K2E2/web...
---------- Networks initialized -------------
[Network G] Total number of parameters : 25.564 M
[Network F] Total number of parameters : 0.675 M
[Network D] Total number of parameters : 2.763 M
-----------------------------------------------
End of epoch 1 / 100 	 Time Taken: 26 sec
learning rate = 0.0001000
End of epoch 2 / 100 	 Time Taken: 15 sec
learning rate = 0.0001000
End of epoch 3 / 100 	 Time Taken: 12 sec
learning rate = 0.0001000
End of epoch 4 / 100 	 Time Taken: 7 sec
learning rate = 0.0001000
End of epoch 5 / 100 	 Time Taken: 10 sec
learning rate = 0.0001000
End of epoch 6 / 100 	 Time Taken: 12 sec
learning rate = 0.0001000
End of epoch 7 / 100 	 Time Taken: 7 sec
learning rate = 0.0001000
End of epoch 8 / 100 	 Time Taken: 11 sec
learning rate = 0.0001000
End of epoch 9 / 100 	 Time Taken: 13 sec
learning rate = 0.0001000
End of epoch 10 / 100 	 Time Taken: 11 sec
learning rate = 0.0001000
End of epoch 11 / 100 	 Time Taken: 10 sec
learning rate = 0.0001000
End of epoch 12 / 100 	 Time Taken: 12 sec
learning rate = 0.0001000
