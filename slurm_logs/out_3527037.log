----------------- Options ---------------
                 CUT_mode: CUT                           
               batch_size: 16                            	[default: 1]
                    beta1: 0.5                           
                    beta2: 0.999                         
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 256                           
                 csv_path: /public_bme2/bme-wangqian2/songhy2024/harmonization_mri/dataset/PVWMI_data.csv	[default: data.csv]
                 dataroot: /public_bme2/bme-wangqian2/songhy2024/data/PVWMI/T1w/k2I-SIEMENS-SKYRA-3.0T/	[default: placeholder]
             dataset_mode: monai                         	[default: unaligned]
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: 0                             	[default: None]
            display_ncols: 4                             
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
               easy_label: experiment_name               
                    epoch: latest                        
              epoch_count: 1                             
          evaluation_freq: 5000                          
        flip_equivariance: False                         
                 gan_mode: lsgan                         
                  gpu_ids: 0                             
                init_gain: 0.02                          
                init_type: xavier                        
                 input_nc: 1                             	[default: 3]
                  isTrain: True                          	[default: None]
               lambda_GAN: 1.0                           
               lambda_NCE: 1.0                           
              lambda_SSIM: 1.0                           
              lambda_grad: 0.5                           
                load_size: 256                           	[default: 286]
                       lr: 0.0001                        	[default: 0.0002]
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: cut                           
                 n_epochs: 50                            	[default: 200]
           n_epochs_decay: 50                            	[default: 200]
               n_layers_D: 3                             
                     name: CUT_monai_K2I                 	[default: experiment_name]
                    nce_T: 0.07                          
                  nce_idt: True                          
nce_includes_all_negatives_from_minibatch: False                         
               nce_layers: 0,4,8,12,16                   
                      ndf: 64                            
                     netD: basic                         
                     netF: mlp_sample                    
                  netF_nc: 256                           
                     netG: resnet_9blocks                
                      ngf: 96                            	[default: 64]
             no_antialias: False                         
          no_antialias_up: False                         
               no_dropout: True                          
                  no_flip: True                          	[default: False]
                  no_html: False                         
                    normD: instance                      
                    normG: instance                      
              num_patches: 256                           
              num_threads: 8                             	[default: 4]
                output_nc: 1                             	[default: 3]
                    phase: train                         
                pool_size: 0                             
               preprocess: scale_width_and_crop          	[default: resize_and_crop]
          pretrained_name: None                          
               print_freq: 100                           
         random_scale_max: 3.0                           
             save_by_iter: False                         
          save_epoch_freq: 20                            	[default: 5]
         save_latest_freq: 5000                          
           serial_batches: False                         
stylegan2_G_num_downsampling: 1                             
                   suffix:                               
         update_html_freq: 1000                          
                  verbose: False                         
----------------- End -------------------
Found 61 files in domain A (thick slices) and 51 files in domain B (thin slices)
Volume cache size: 100
Dataset will output 1 channel(s) per slice
Domain A: 4 CP=1, 57 CP=2
Domain B: 43 CP=1, 8 CP=2
Calculating valid slices for each domain...
Domain A (thick) total valid slices: 6123
Domain B (thin) total valid slices: 678
dataset [MonaiDataset] was created
CPSampler initialized: len_a=57, len_b=8, total_len=130
model [CUTModel] was created
The number of training images = 6123
create web directory ./checkpoints/CUT_monai_K2I/web...
---------- Networks initialized -------------
[Network G] Total number of parameters : 25.564 M
[Network F] Total number of parameters : 0.675 M
[Network D] Total number of parameters : 2.763 M
-----------------------------------------------
End of epoch 1 / 100 	 Time Taken: 25 sec
learning rate = 0.0001000
End of epoch 2 / 100 	 Time Taken: 10 sec
learning rate = 0.0001000
End of epoch 3 / 100 	 Time Taken: 14 sec
learning rate = 0.0001000
End of epoch 4 / 100 	 Time Taken: 14 sec
learning rate = 0.0001000
End of epoch 5 / 100 	 Time Taken: 13 sec
learning rate = 0.0001000
End of epoch 6 / 100 	 Time Taken: 14 sec
learning rate = 0.0001000
End of epoch 7 / 100 	 Time Taken: 8 sec
learning rate = 0.0001000
End of epoch 8 / 100 	 Time Taken: 8 sec
learning rate = 0.0001000
End of epoch 9 / 100 	 Time Taken: 10 sec
learning rate = 0.0001000
End of epoch 10 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
End of epoch 11 / 100 	 Time Taken: 17 sec
learning rate = 0.0001000
End of epoch 12 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
(epoch: 13, iters: 16, time: 0.104, data: 4.493) G_GAN: 0.295 D_real: 0.297 D_fake: 0.230 G: 7.948 NCE: 7.096 Idt: 0.518 SSIM: 0.388 Grad: 0.159 NCE_Y: 7.115 
End of epoch 13 / 100 	 Time Taken: 10 sec
learning rate = 0.0001000
End of epoch 14 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
End of epoch 15 / 100 	 Time Taken: 11 sec
learning rate = 0.0001000
End of epoch 16 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
End of epoch 17 / 100 	 Time Taken: 11 sec
learning rate = 0.0001000
End of epoch 18 / 100 	 Time Taken: 11 sec
learning rate = 0.0001000
End of epoch 19 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
saving the model at the end of epoch 20, iters 640
End of epoch 20 / 100 	 Time Taken: 17 sec
learning rate = 0.0001000
End of epoch 21 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
End of epoch 22 / 100 	 Time Taken: 11 sec
learning rate = 0.0001000
End of epoch 23 / 100 	 Time Taken: 11 sec
learning rate = 0.0001000
End of epoch 24 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
(epoch: 25, iters: 32, time: 0.102, data: 0.001) G_GAN: 0.267 D_real: 0.270 D_fake: 0.246 G: 7.620 NCE: 6.970 Idt: 0.342 SSIM: 0.251 Grad: 0.147 NCE_Y: 6.939 
End of epoch 25 / 100 	 Time Taken: 14 sec
learning rate = 0.0001000
End of epoch 26 / 100 	 Time Taken: 7 sec
learning rate = 0.0001000
End of epoch 27 / 100 	 Time Taken: 7 sec
learning rate = 0.0001000
End of epoch 28 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
End of epoch 29 / 100 	 Time Taken: 8 sec
learning rate = 0.0001000
End of epoch 30 / 100 	 Time Taken: 7 sec
learning rate = 0.0001000
End of epoch 31 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
End of epoch 32 / 100 	 Time Taken: 8 sec
learning rate = 0.0001000
End of epoch 33 / 100 	 Time Taken: 10 sec
learning rate = 0.0001000
End of epoch 34 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
End of epoch 35 / 100 	 Time Taken: 11 sec
learning rate = 0.0001000
End of epoch 36 / 100 	 Time Taken: 6 sec
learning rate = 0.0001000
End of epoch 37 / 100 	 Time Taken: 8 sec
learning rate = 0.0001000
(epoch: 38, iters: 16, time: 0.101, data: 4.332) G_GAN: 0.283 D_real: 0.311 D_fake: 0.229 G: 7.102 NCE: 6.538 Idt: 0.304 SSIM: 0.188 Grad: 0.131 NCE_Y: 6.460 
End of epoch 38 / 100 	 Time Taken: 7 sec
learning rate = 0.0001000
End of epoch 39 / 100 	 Time Taken: 8 sec
learning rate = 0.0001000
saving the model at the end of epoch 40, iters 1280
End of epoch 40 / 100 	 Time Taken: 12 sec
learning rate = 0.0001000
End of epoch 41 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
End of epoch 42 / 100 	 Time Taken: 7 sec
learning rate = 0.0001000
End of epoch 43 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
End of epoch 44 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
End of epoch 45 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
End of epoch 46 / 100 	 Time Taken: 9 sec
learning rate = 0.0001000
End of epoch 47 / 100 	 Time Taken: 7 sec
learning rate = 0.0001000
End of epoch 48 / 100 	 Time Taken: 11 sec
learning rate = 0.0001000
End of epoch 49 / 100 	 Time Taken: 7 sec
learning rate = 0.0001000
(epoch: 50, iters: 32, time: 0.100, data: 0.003) G_GAN: 0.260 D_real: 0.258 D_fake: 0.258 G: 6.886 NCE: 6.370 Idt: 0.238 SSIM: 0.168 Grad: 0.132 NCE_Y: 6.281 
End of epoch 50 / 100 	 Time Taken: 10 sec
learning rate = 0.0000980
End of epoch 51 / 100 	 Time Taken: 10 sec
learning rate = 0.0000961
End of epoch 52 / 100 	 Time Taken: 9 sec
learning rate = 0.0000941
End of epoch 53 / 100 	 Time Taken: 10 sec
learning rate = 0.0000922
End of epoch 54 / 100 	 Time Taken: 8 sec
learning rate = 0.0000902
End of epoch 55 / 100 	 Time Taken: 10 sec
learning rate = 0.0000882
End of epoch 56 / 100 	 Time Taken: 9 sec
learning rate = 0.0000863
End of epoch 57 / 100 	 Time Taken: 7 sec
learning rate = 0.0000843
End of epoch 58 / 100 	 Time Taken: 16 sec
learning rate = 0.0000824
End of epoch 59 / 100 	 Time Taken: 10 sec
learning rate = 0.0000804
saving the model at the end of epoch 60, iters 1920
End of epoch 60 / 100 	 Time Taken: 20 sec
learning rate = 0.0000784
End of epoch 61 / 100 	 Time Taken: 10 sec
learning rate = 0.0000765
End of epoch 62 / 100 	 Time Taken: 12 sec
learning rate = 0.0000745
(epoch: 63, iters: 16, time: 0.100, data: 7.512) G_GAN: 0.265 D_real: 0.248 D_fake: 0.264 G: 6.187 NCE: 5.682 Idt: 0.186 SSIM: 0.152 Grad: 0.127 NCE_Y: 5.605 
End of epoch 63 / 100 	 Time Taken: 13 sec
learning rate = 0.0000725
End of epoch 64 / 100 	 Time Taken: 9 sec
learning rate = 0.0000706
End of epoch 65 / 100 	 Time Taken: 7 sec
learning rate = 0.0000686
End of epoch 66 / 100 	 Time Taken: 10 sec
learning rate = 0.0000667
End of epoch 67 / 100 	 Time Taken: 8 sec
learning rate = 0.0000647
End of epoch 68 / 100 	 Time Taken: 14 sec
learning rate = 0.0000627
End of epoch 69 / 100 	 Time Taken: 10 sec
learning rate = 0.0000608
End of epoch 70 / 100 	 Time Taken: 10 sec
learning rate = 0.0000588
End of epoch 71 / 100 	 Time Taken: 8 sec
learning rate = 0.0000569
End of epoch 72 / 100 	 Time Taken: 10 sec
learning rate = 0.0000549
End of epoch 73 / 100 	 Time Taken: 9 sec
learning rate = 0.0000529
End of epoch 74 / 100 	 Time Taken: 22 sec
learning rate = 0.0000510
(epoch: 75, iters: 32, time: 0.099, data: 0.003) G_GAN: 0.272 D_real: 0.286 D_fake: 0.234 G: 5.791 NCE: 5.288 Idt: 0.182 SSIM: 0.120 Grad: 0.112 NCE_Y: 5.286 
End of epoch 75 / 100 	 Time Taken: 14 sec
learning rate = 0.0000490
End of epoch 76 / 100 	 Time Taken: 10 sec
learning rate = 0.0000471
End of epoch 77 / 100 	 Time Taken: 8 sec
learning rate = 0.0000451
End of epoch 78 / 100 	 Time Taken: 13 sec
learning rate = 0.0000431
End of epoch 79 / 100 	 Time Taken: 11 sec
learning rate = 0.0000412
saving the model at the end of epoch 80, iters 2560
End of epoch 80 / 100 	 Time Taken: 49 sec
learning rate = 0.0000392
End of epoch 81 / 100 	 Time Taken: 15 sec
learning rate = 0.0000373
End of epoch 82 / 100 	 Time Taken: 9 sec
learning rate = 0.0000353
End of epoch 83 / 100 	 Time Taken: 15 sec
learning rate = 0.0000333
End of epoch 84 / 100 	 Time Taken: 8 sec
learning rate = 0.0000314
End of epoch 85 / 100 	 Time Taken: 10 sec
learning rate = 0.0000294
End of epoch 86 / 100 	 Time Taken: 10 sec
learning rate = 0.0000275
End of epoch 87 / 100 	 Time Taken: 11 sec
learning rate = 0.0000255
(epoch: 88, iters: 16, time: 0.098, data: 5.770) G_GAN: 0.246 D_real: 0.276 D_fake: 0.265 G: 5.422 NCE: 4.900 Idt: 0.170 SSIM: 0.122 Grad: 0.120 NCE_Y: 4.966 
End of epoch 88 / 100 	 Time Taken: 21 sec
learning rate = 0.0000235
End of epoch 89 / 100 	 Time Taken: 11 sec
learning rate = 0.0000216
End of epoch 90 / 100 	 Time Taken: 16 sec
learning rate = 0.0000196
End of epoch 91 / 100 	 Time Taken: 11 sec
learning rate = 0.0000176
End of epoch 92 / 100 	 Time Taken: 15 sec
learning rate = 0.0000157
End of epoch 93 / 100 	 Time Taken: 9 sec
learning rate = 0.0000137
End of epoch 94 / 100 	 Time Taken: 11 sec
learning rate = 0.0000118
End of epoch 95 / 100 	 Time Taken: 12 sec
learning rate = 0.0000098
End of epoch 96 / 100 	 Time Taken: 8 sec
learning rate = 0.0000078
End of epoch 97 / 100 	 Time Taken: 10 sec
learning rate = 0.0000059
End of epoch 98 / 100 	 Time Taken: 10 sec
learning rate = 0.0000039
End of epoch 99 / 100 	 Time Taken: 10 sec
learning rate = 0.0000020
(epoch: 100, iters: 32, time: 0.098, data: 0.003) G_GAN: 0.263 D_real: 0.275 D_fake: 0.244 G: 5.288 NCE: 4.825 Idt: 0.184 SSIM: 0.113 Grad: 0.103 NCE_Y: 4.793 
saving the model at the end of epoch 100, iters 3200
End of epoch 100 / 100 	 Time Taken: 51 sec
learning rate = 0.0000000
